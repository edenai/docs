Status: 200
Content-Type: text/event-stream
---BODY---
data: {"type":"start","messageId":"gRiN4ueppMzEk5Aw"}

data: {"type":"start-step"}

data: {"type":"tool-input-start","toolCallId":"toolu_01BERnZhujydgkFfAbH9b1Jt","toolName":"search"}

data: {"type":"tool-input-delta","toolCallId":"toolu_01BERnZhujydgkFfAbH9b1Jt","inputTextDelta":""}

data: {"type":"tool-input-delta","toolCallId":"toolu_01BERnZhujydgkFfAbH9b1Jt","inputTextDelta":"{\"query\": "}

data: {"type":"tool-input-delta","toolCallId":"toolu_01BERnZhujydgkFfAbH9b1Jt","inputTextDelta":"\"chat co"}

data: {"type":"tool-input-delta","toolCallId":"toolu_01BERnZhujydgkFfAbH9b1Jt","inputTextDelta":"mpletion"}

data: {"type":"tool-input-delta","toolCallId":"toolu_01BERnZhujydgkFfAbH9b1Jt","inputTextDelta":" request"}

data: {"type":"tool-input-delta","toolCallId":"toolu_01BERnZhujydgkFfAbH9b1Jt","inputTextDelta":"\"}"}

data: {"type":"tool-input-available","toolCallId":"toolu_01BERnZhujydgkFfAbH9b1Jt","toolName":"search","input":{"query":"chat completion request"},"providerMetadata":{"openrouter":{"reasoning_details":[]}}}

data: {"type":"tool-output-available","toolCallId":"toolu_01BERnZhujydgkFfAbH9b1Jt","output":{"type":"search","results":[{"content":"","path":"api-reference/completions/chat-completions","metadata":{"title":"Chat Completions","description":"OpenAI-compatible chat completions endpoint (v3).","deprecated":null,"version":null,"openapi":"openapi/v3-openapi.json post /v3/llm/chat/completions","href":"/api-reference/completions/chat-completions","autogeneratedByOpenApi":true,"hidden":false}},{"content":"# Getting Started with Eden AI V3 API\n\nWelcome to the Eden AI V3 API! This guide will help you understand V3's unified architecture and make your first API calls.\n\n## Overview\n\nEden AI V3 introduces a revolutionary approach to AI API integration with:\n\n* **Universal AI Endpoint** - Single endpoint for all non-LLM features\n* **OpenAI-Compatible Format** - Drop-in replacement for OpenAI's API\n* **Persistent File Storage** - Upload once, use in multiple requests\n* **Built-in API Discovery** - Explore features and schemas programmatically\n* **SSE for Streaming** - When streaming is enabled, LLM responses use Server-Sent Events (SSE). Streaming is optional.\n\n<Note>If you were an user before 2026/01/05, you still have access to the previous version: [https://old-app.edenai.run/](https://old-app.edenai.run/). We'll continue supporting the old version until the end of 2026. It your're looking for the documentation, you can find it [here](https://old-docs.edenai.co)</Note>\n\n## V3 Architecture\n\nV3 uses a **model string** format instead of separate provider parameters:\n\n```\nfeature/subfeature/provider[/model]\n```\n\n**Examples:**\n\n* `text/moderation/google`\n* `ocr/financial_parser/google`\n* `image/generation/openai/dall-e-3`\n* `openai/gpt-4` (for LLM endpoints)\n\nThis unified format allows a single endpoint to handle all features intelligently.\n\n## Prerequisites\n\nBefore you start, you'll need:\n\n1. **API Token** - Get your token from the [Eden AI dashboard](https://app.edenai.run/)\n2. **HTTP Client** - Use cURL, Python requests, or any HTTP client\n3. **Credits** - Ensure your account has sufficient credits\n\n## Base URL\n\nAll V3 API endpoints are available at:\n\n```\nhttps://api.edenai.run/v3\n```\n\n## Authentication\n\nAll requests must include your API token in the Authorization header:\n\n<CodeGroup>\n  ```bash cURL theme={null}\n  curl -X POST https://api.edenai.run/v3/universal-ai \\\n    -H \"Authorization: Bearer YOUR_API_KEY\" \\\n    -H \"Content-Type: application/json\"\n  ```\n\n  ```python Python theme={null}\n  import requests\n\n  headers = {\n      \"Authorization\": \"Bearer YOUR_API_KEY\",\n      \"Content-Type\": \"application/json\"\n  }\n\n  response = requests.post(\n      \"https://api.edenai.run/v3/universal-ai\",\n      headers=headers,\n      json={\"model\": \"text/moderation/openai\", \"input\": {\"text\": \"Sample\"}}\n  )\n  ```\n\n  ```javascript JavaScript theme={null}\n  const headers = {\n    'Authorization': 'Bearer YOUR_API_KEY',\n    'Content-Type': 'application/json'\n  };\n\n  const response = await fetch('https://api.edenai.run/v3/universal-ai', {\n    method: 'POST',\n    headers: headers,\n    body: JSON.stringify({\n      model: 'text/moderation/openai',\n      input: { text: 'Sample' }\n    })\n  });\n  ```\n</CodeGroup>\n\n## Your First API Call: Universal AI\n\nThe Universal AI endpoint handles all non-LLM features through a single endpoint. Let's moderate some text:\n\n<CodeGroup>\n  ```bash cURL theme={null}\n  curl -X POST https://api.edenai.run/v3/universal-ai \\\n    -H \"Authorization: Bearer YOUR_API_KEY\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n      \"model\": \"text/moderation/google\",\n      \"input\": {\n        \"text\": \"This is a sample text to moderate for harmful content.\"\n      }\n    }'\n  ```\n\n  ```python Python theme={null}\n  import requests\n\n  url = \"https://api.edenai.run/v3/universal-ai\"\n  headers = {\n      \"Authorization\": \"Bearer YOUR_API_KEY\",\n      \"Content-Type\": \"application/json\"\n  }\n  payload = {\n      \"model\": \"text/moderation/google\",\n      \"input\": {\n          \"text\": \"This is a sample text to moderate for harmful content.\"\n      }\n  }\n\n  response = requests.post(url, headers=headers, json=payload)\n  result = response.json()\n  print(result)\n  ```\n\n  ```javascript JavaScript theme={null}\n  const url = 'https://api.edenai.run/v3/universal-ai';\n  const headers = {\n    'Authorization': 'Bearer YOUR_API_KEY',\n    'Content-Type': 'application/json'\n  };\n  const payload = {\n    model: 'text/moderation/google',\n    input: {\n      text: 'This is a sample text to moderate for harmful content.'\n    }\n  };\n\n  const response = await fetch(url, {\n    method: 'POST',\n    headers: headers,\n    body: JSON.stringify(payload)\n  });\n\n  const result = await response.json();\n  console.log(result);\n  ```\n</CodeGroup>\n\n### Response Format\n\nAll V3 responses follow a consistent structure:\n\n```json  theme={null}\n{\n  \"status\": \"success\",\n  \"cost\": 0.0001,\n  \"provider\": \"google\",\n  \"feature\": \"text\",\n  \"subfeature\": \"moderation\",\n  \"output\": {\n    \"nsfw_likelihood\": 1,\n    \"items\": [\n      {\"label\": \"Toxic\", \"likelihood\": 1, \"category\": \"Toxic\", \"subcategory\": \"Toxic\", \"likelihood_score\": 0.0908},\n      {\"label\": \"Violent\", \"likelihood\": 1, \"category\": \"Violence\", \"subcategory\": \"Violence\", \"likelihood_score\": 0.0120},\n      {\"label\": \"Sexual\", \"likelihood\": 1, \"category\": \"Sexual\", \"subcategory\": \"Sexual\", \"likelihood_score\": 0.0045}\n    ],\n    \"nsfw_likelihood_score\": 0.0908\n  }\n}\n```\n\n## Your First LLM Call: OpenAI-Compatible\n\nThe LLM endpoint provides OpenAI-compatible chat completions:\n\n<CodeGroup>\n  ```python Python theme={null}\n  import requests\n\n  url = \"https://api.edenai.run/v3/llm/chat/completions\"\n  headers = {\n      \"Authorization\": \"Bearer YOUR_API_KEY\",\n      \"Content-Type\": \"application/json\"\n  }\n  payload = {\n      \"model\": \"openai/gpt-4\",\n      \"messages\": [\n          {\"role\": \"user\", \"content\": \"Hello! How are you?\"}\n      ]\n  }\n\n  response = requests.post(url, headers=headers, json=payload)\n  result = response.json()\n  print(result[\"choices\"][0][\"message\"][\"content\"])\n  ```\n\n  ```bash cURL theme={null}\n  curl -X POST https://api.edenai.run/v3/llm/chat/completions \\\n    -H \"Authorization: Bearer YOUR_API_KEY\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n      \"model\": \"openai/gpt-4\",\n      \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n    }'\n  ```\n\n  ```javascript JavaScript theme={null}\n  const url = 'https://api.edenai.run/v3/llm/chat/completions';\n  const headers = {\n    'Authorization': 'Bearer YOUR_API_KEY',\n    'Content-Type': 'application/json'\n  };\n  const payload = {\n    model: 'openai/gpt-4',\n    messages: [{role: 'user', content: 'Hello!'}]\n  };\n\n  const response = await fetch(url, {\n    method: 'POST',\n    headers: headers,\n    body: JSON.stringify(payload)\n  });\n\n  const result = await response.json();\n  console.log(result.choices[0].message.content);\n  ```\n</CodeGroup>\n\n### Response Format\n\nLLM responses follow the OpenAI format:\n\n```json  theme={null}\n{\n  \"id\": \"chatcmpl-123\",\n  \"object\": \"chat.completion\",\n  \"choices\": [\n    {\n      \"message\": {\"role\": \"assistant\", \"content\": \"Hello! I'm doing well, thank you.\"},\n      \"index\": 0,\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\"prompt_tokens\": 12, \"completion_tokens\": 10, \"total_tokens\": 22}\n}\n```\n\n## Working with Files\n\nV3 introduces **persistent file storage**. Upload files once, then reference them by ID:\n\n### Step 1: Upload a File\n\n<CodeGroup>\n  ```python Python theme={null}\n  import requests\n\n  url = \"https://api.edenai.run/v3/upload\"\n  headers = {\"Authorization\": \"Bearer YOUR_API_KEY\"}\n\n  files = {\"file\": open(\"document.pdf\", \"rb\")}\n  data = {\"purpose\": \"ocr-processing\"}\n\n  response = requests.post(url, headers=headers, files=files, data=data)\n  file_info = response.json()\n  file_id = file_info[\"file_id\"]\n\n  print(f\"Uploaded file ID: {file_id}\")\n  # Output: Uploaded file ID: 550e8400-e29b-41d4-a716-446655440000\n  ```\n\n  ```bash cURL theme={null}\n  curl -X POST https://api.edenai.run/v3/upload \\\n    -H \"Authorization: Bearer YOUR_API_KEY\" \\\n    -F \"file=@document.pdf\" \\\n    -F \"purpose=ocr-processing\"\n  ```\n</CodeGroup>\n\n### Step 2: Use the File in Requests\n\n<CodeGroup>\n  ```python Python theme={null}\n  import requests\n  # Use the file_id in a Universal AI request\n  url = \"https://api.edenai.run/v3/universal-ai\"\n  headers = {\n      \"Authorization\": \"Bearer YOUR_API_KEY\",\n      \"Content-Type\": \"application/json\"\n  }\n  payload = {\n      \"model\": \"ocr/financial_parser/google\",\n      \"input\": {\n          \"file\": \"550e8400-e29b-41d4-a716-446655440000\"\n      }\n  }\n\n  response = requests.post(url, headers=headers, json=payload)\n  result = response.json()\n  print(result[\"output\"][\"text\"])\n  ```\n</CodeGroup>\n\n## Understanding Model Strings\n\nThe model string is the key to V3's unified architecture. Here's the format:\n\n```\nfeature/subfeature/provider[/model]\n```\n\n### Breaking It Down\n\n| Component    | Description               | Example                                           |\n| ------------ | ------------------------- | ------------------------------------------------- |\n| `feature`    | Category of AI capability | `text`, `ocr`, `image`, `translation`             |\n| `subfeature` | Specific functionality    | `moderation`, `ai_detection`, `ocr`, `generation` |\n| `provider`   | AI provider               | `openai`, `google`, `amazon`, `anthropic`         |\n| `model`      | Specific model (optional) | `gpt-4`, `claude-sonnet-4-5`, `gemini-pro`        |\n\n### Examples\n\n**Text Analysis:**\n\n* `text/moderation/google` - Content moderation with Google\n* `text/moderation/openai` - Content moderation with OpenAI's default model\n* `text/embeddings/cohere/embed-english-v3.0` - Generate embeddings with Cohere\n\n**OCR:**\n\n* `ocr/financial_parser/google` - Extract financial information with Google DocumentAI\n* `ocr/identity_parser/amazon` - Parse ID documents with Amazon Textract\n* `ocr/invoice_parser/microsoft` - Extract invoice data with Azure\n\n**Image:**\n\n* `image/generation/openai/dall-e-3` - Generate images with DALL-E 3\n* `image/object_detection/google` - Detect objects with Google Vision\n* `image/face_detection/amazon` - Detect faces with AWS Rekognition\n\n**LLM (simplified format):**\n\n* `openai/gpt-4` - Chat with GPT-4\n* `anthropic/claude-sonnet-4-5` - Chat with Claude\n* `google/gemini-pro` - Chat with Gemini\n\n## API Discovery\n\nV3 includes built-in endpoints to explore available features programmatically:\n\n<CodeGroup>\n  ```python Python theme={null}\n  import requests\n\n  # List all features\n  response = requests.get(\n      \"https://api.edenai.run/v3/info\"\n  )\n  features = response.json()\n  print(features)\n  # Output: {\"text\": [\"moderation\", \"ai_detection\", ...], \"ocr\": [...], ...}\n\n  # Get details about a specific feature\n  response = requests.get(\n      \"https://api.edenai.run/v3/info/text/moderation\"\n  )\n  feature_info = response.json()\n  print(feature_info[\"providers\"])  # List of available providers\n  print(feature_info[\"input_schema\"])  # Expected input format\n  print(feature_info[\"output_schema\"])  # Response format\n  ```\n\n  ```bash cURL theme={null}\n  # List all features\n  curl https://api.edenai.run/v3/info \n  # Get feature details\n  curl https://api.edenai.run/v3/info/text/moderation\n  ```\n</CodeGroup>\n\n## Error Handling\n\nV3 uses standard HTTP status codes with detailed error messages:\n\n```json  theme={null}\n{\n  \"status\": \"error\",\n  \"error\": {\n    \"code\": \"invalid_model_string\",\n    \"message\": \"Model string format must be feature/subfeature/provider[/model]\",\n    \"details\": {\n      \"provided\": \"invalid/model\",\n      \"expected\": \"feature/subfeature/provider[/model]\"\n    }\n  }\n}\n```\n\n### Common Status Codes\n\n* `200` - Success\n* `400` - Bad Request (invalid model string or input)\n* `401` - Unauthorized (invalid API token)\n* `402` - Payment Required (insufficient credits)\n* `404` - Not Found (feature/provider not available)\n* `422` - Validation Error (invalid request body)\n* `429` - Rate Limit Exceeded\n* `500` - Internal Server Error\n\n## V3 vs V2: Key Differences\n\n| Aspect                   | V2                                       | V3                                      |\n| ------------------------ | ---------------------------------------- | --------------------------------------- |\n| **Endpoints**            | Feature-specific (`/v2/text/moderation`) | Universal (`/v3/universal-ai`) + LLM    |\n| **Provider Format**      | `providers` parameter                    | Model string (`text/moderation/openai`) |\n| **File Handling**        | Per-request uploads                      | Persistent storage with file IDs        |\n| **LLM Streaming**        | Optional                                 | Optional (SSE when enabled)             |\n| **API Discovery**        | Documentation only                       | Built-in `/v3/info` endpoints           |\n| **OpenAI Compatibility** | Custom format                            | Native OpenAI format                    |\n\n## Next Steps\n\nNow that you understand V3 basics, explore specific features:\n\n### Universal AI\n\n* [Getting Started with Universal AI](../how-to/universal-ai/getting-started) - Learn the universal endpoint\n* [Text Features](../how-to/universal-ai/text-features) - AI detection, moderation, embeddings\n* [OCR Features](../how-to/universal-ai/ocr-features) - Text extraction, document parsing\n* [Image Features](../how-to/universal-ai/image-features) - Generation, detection, analysis\n\n### OpenAI-Compatible LLM\n\n* [Chat Completions](../how-to/llm/chat-completions) - Build conversational AI with streaming\n* [Streaming Responses](../how-to/llm/streaming) - Handle Server-Sent Events\n* [File Attachments](../how-to/llm/file-attachments) - Send images and documents to LLMs\n\n### File Management\n\n* [Upload Files](../how-to/upload/upload-files) - Persistent file storage\n\n### API Discovery\n\n* [Explore the API](../how-to/discovery/explore-api) - Programmatic feature discovery\n\n## Need Help?\n\n* Visit [Eden AI Support](https://edenai.co/) for additional assistance\n","path":"v3/get-started/introduction","metadata":{"sidebarTitle":"Introduction","icon":"book-open","title":null,"description":null,"href":"/v3/get-started/introduction","hidden":false}},{"content":"# Smart Routing: Let AI Choose the Best Model\n\nGet optimal model selection automatically with Eden AI's smart routing feature. Instead of manually selecting models, let the system intelligently route your requests based on context.\n\n## Overview\n\nSmart routing uses the special model identifier `@edenai` to dynamically select the best LLM model for your request. The system analyzes your messages, tools, and other context to route to the optimal provider/model combination.\n\n**Key Benefits:**\n\n* **Automatic optimization** - No need to research and compare models\n* **Cost efficiency** - Routes to models with the best price/performance ratio\n* **Context-aware** - Selection adapts to your specific request\n* **Fallback resilience** - Automatic fallback if routing fails\n* **Works with all features** - Streaming, function calling, vision, etc.\n\n**Powered by:** [NotDiamond](https://notdiamond.ai/) routing engine\n\n## Quick Start: Your First Routed Request\n\nThe simplest way to use smart routing is to set `model: \"@edenai\"`:\n\n<CodeGroup>\n  ```python Python theme={null}\n  import requests\n\n  url = \"https://api.edenai.run/v3/llm/chat/completions\"\n  headers = {\n      \"Authorization\": \"Bearer YOUR_API_KEY\",\n      \"Content-Type\": \"application/json\"\n  }\n\n  payload = {\n      \"model\": \"@edenai\",  # Let Eden AI choose the best model\n      \"messages\": [\n          {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms\"}\n      ]\n  }\n\n  # The system automatically selects the optimal model\n  response = requests.post(url, headers=headers, json=payload)\n\n  print(response.json())\n  ```\n\n  ```javascript JavaScript theme={null}\n  const url = 'https://api.edenai.run/v3/llm/chat/completions';\n  const headers = {\n    'Authorization': 'Bearer YOUR_API_KEY',\n    'Content-Type': 'application/json'\n  };\n\n  const payload = {\n    model: '@edenai',  // Let Eden AI choose the best model\n    messages: [\n      {role: 'user', content: 'Explain quantum computing in simple terms'}\n    ]\n  };\n\n  const response = await fetch(url, {\n    method: 'POST',\n    headers: headers,\n    body: JSON.stringify(payload)\n  });\n\n  const data = await response.json();\n  console.log(data);\n  ```\n\n  ```bash cURL theme={null}\n  curl -X POST https://api.edenai.run/v3/llm/chat/completions \\\n    -H \"Authorization: Bearer YOUR_API_KEY\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n      \"model\": \"@edenai\",\n      \"messages\": [\n        {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms\"}\n      ]\n    }'\n  ```\n</CodeGroup>\n\nThat's it! The system will automatically select the best model from all available options.\n\n## How It Works\n\nWhen you use `model: \"@edenai\"`, here's what happens:\n\n1. **Request Analysis** - The system analyzes your messages, tools, and parameters\n2. **Model Selection** - NotDiamond's routing engine selects the optimal model from available candidates\n3. **Transparent Routing** - Your request is routed to the selected model\n4. **Normal Response** - You receive the response as if you had specified the model directly\n\n**Typical latency:** 100-500ms additional processing time for model selection\n\n## Customizing Model Candidates\n\nBy default, smart routing considers all available models. You can customize the candidate pool with `router_candidates`:\n\n<CodeGroup>\n  ```python Python theme={null}\n  import requests\n\n  url = \"https://api.edenai.run/v3/llm/chat/completions\"\n  headers = {\n      \"Authorization\": \"Bearer YOUR_API_KEY\",\n      \"Content-Type\": \"application/json\"\n  }\n\n  payload = {\n      \"model\": \"@edenai\",\n      \"router_candidates\": [\n          \"openai/gpt-4o\",\n          \"anthropic/claude-sonnet-4-5\",\n          \"google/gemini-2.0-flash\"\n      ],\n      \"messages\": [\n          {\"role\": \"user\", \"content\": \"Write a Python function to sort a list\"}\n      ]\n  }\n\n  # Router will choose from only these 3 models\n  response = requests.post(url, headers=headers, json=payload)\n\n  print(response.json())\n  ```\n\n  ```javascript JavaScript theme={null}\n  const payload = {\n    model: '@edenai',\n    router_candidates: [\n      'openai/gpt-4o',\n      'anthropic/claude-sonnet-4-5',\n      'google/gemini-2.0-flash'\n    ],\n    messages: [\n      {role: 'user', content: 'Write a Python function to sort a list'}\n    ]\n  };\n\n  const response = await fetch('https://api.edenai.run/v3/llm/chat/completions', {\n    method: 'POST',\n    headers: {\n      'Authorization': 'Bearer YOUR_API_KEY',\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify(payload)\n  });\n  ```\n\n  ```bash cURL theme={null}\n  curl -X POST https://api.edenai.run/v3/llm/chat/completions \\\n    -H \"Authorization: Bearer YOUR_API_KEY\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n      \"model\": \"@edenai\",\n      \"router_candidates\": [\n        \"openai/gpt-4o\",\n        \"anthropic/claude-sonnet-4-5\",\n        \"google/gemini-2.0-flash\"\n      ],\n      \"messages\": [\n        {\"role\": \"user\", \"content\": \"Write a Python function to sort a list\"}\n      ]\n    }'\n  ```\n</CodeGroup>\n\n## When to Use Smart Routing\n\n### ✅ Great Use Cases\n\n* **General-purpose chatbots** - Let the router optimize for diverse queries\n* **Cost-sensitive applications** - Router balances quality and cost\n* **Multi-task applications** - Different queries benefit from different models\n* **Experimentation** - Compare router performance vs. fixed models\n\n### ⚠️ Consider Fixed Models Instead\n\n* **Specific model requirements** - You need a particular model's unique features\n* **Latency-critical** - Every 100ms matters (smart routing adds overhead)\n* **Consistent behavior** - You need identical model behavior across requests\n* **High-volume production** - You've already optimized model selection\n\n## Model Selection Criteria\n\nThe routing engine considers multiple factors:\n\n* **Task type** - Code generation, creative writing, analysis, etc.\n* **Conversation context** - Prior messages and conversation flow\n* **Tool/function calls** - Compatibility with function calling requirements\n* **Quality requirements** - Implicit in message complexity\n* **Cost efficiency** - Price/performance optimization\n\n## Default Model Pool\n\nWhen you don't specify `router_candidates`, the system uses all available LLM models, including:\n\n**Top-tier Models:**\n\n* OpenAI: GPT-4, GPT-4 Turbo, GPT-4o, GPT-5 (latest versions)\n* Anthropic: Claude Haiku, Sonnet, Opus (4.x series)\n* Google: Gemini 2.0/2.5/3.0 (Flash and Pro)\n* Mistral: Large, Medium, Small models\n\n**Specialized Models:**\n\n* X.AI: Grok 3, Grok 4\n* Together.ai: Llama 3/3.1 models\n* Cohere: Command R, Command R+\n* Perplexity: Sonar\n\nThe exact pool is dynamically managed and may change as new models become available.\n\n## Fallback Behavior\n\nSmart routing includes intelligent fallbacks:\n\n1. **With custom candidates** - Falls back to first candidate on routing failure\n2. **Without candidates** - Falls back to `openai/gpt-4o` (reliable default)\n3. **Transparent errors** - You'll see clear error messages if routing fails completely\n\n<Info>\n  **Reliability:** Fallback ensures your requests always succeed, even if the routing service is temporarily unavailable.\n</Info>\n\n## Response Format\n\nSmart routing responses are identical to fixed-model responses. The streaming format follows OpenAI's SSE standard:\n\n```\ndata: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"openai/gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"Hello\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"openai/gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"!\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"openai/gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"stop\"}]}\n\ndata: [DONE]\n```\n\n<Tip>\n  **Tracking:** The `model` field in the response shows which model was selected by the router.\n</Tip>\n\n## OpenAI SDK Compatibility\n\nSmart routing works seamlessly with the OpenAI Python SDK:\n\n<CodeGroup>\n  ```python Python (OpenAI SDK) theme={null}\n  from openai import OpenAI\n\n  client = OpenAI(\n      api_key=\"YOUR_API_KEY\",\n      base_url=\"https://api.edenai.run/v3/llm\"\n  )\n\n  # Use smart routing with OpenAI SDK\n  response = client.chat.completions.create(\n      model=\"@edenai\",\n      messages=[\n          {\"role\": \"user\", \"content\": \"Tell me a joke\"}\n      ],\n      extra_body={  # Custom parameters\n          \"router_candidates\": [\n              \"openai/gpt-4o\",\n              \"anthropic/claude-sonnet-4-5\"\n          ]\n      }\n  )\n\n  print(response.choices[0].message.content)\n  ```\n</CodeGroup>\n\n<Info>\n  **SDK Integration:** Use `extra_body` to pass the `router_candidates` parameter when using the OpenAI SDK.\n</Info>\n\n## Pricing\n\nSmart routing costs are based on the **selected model's pricing**. The routing decision itself is free.\n\n**Example:**\n\n* If router selects `openai/gpt-4o` → You pay GPT-4o rates\n* If router selects `google/gemini-2.0-flash` → You pay Gemini Flash rates\n\nThe router optimizes for cost-effectiveness, often selecting cheaper models when they meet quality requirements.\n\n## Next Steps\n\nNow that you understand smart routing basics:\n\n* **[Smart Routing How-To Guide](../how-to/llm/smart-routing)** - Advanced patterns and best practices\n* **[Optimize LLM Costs Tutorial](../tutorials/optimize-llm-costs)** - Complete cost optimization workflow\n* **[Streaming Guide](../how-to/llm/streaming)** - Handle SSE responses effectively\n* **[Function Calling](../how-to/llm/chat-completions#function-calling)** - Use tools with smart routing\n\n## Troubleshooting\n\n### Router returns error \"no candidates provided\"\n\n**Cause:** Empty `router_candidates` list or all candidates filtered out\n**Solution:** Omit `router_candidates` to use default pool, or provide valid model strings\n\n### Higher latency than expected\n\n**Cause:** Routing decision adds 100-500ms overhead\n**Solution:** Use fixed models for latency-critical applications\n\n### Unexpected model selection\n\n**Cause:** Router optimizes for multiple factors, not just quality\n**Solution:** Use `router_candidates` to limit selection pool, or switch to fixed models\n\n### \"Router API unavailable\" errors\n\n**Cause:** Temporary routing service outage\n**Solution:** System automatically falls back - check if fallback model meets your needs\n\n<Tip>\n  **Getting Started:** Start with default routing (`model: \"@edenai\"`), then customize with `router_candidates` if needed.\n</Tip>\n","path":"v3/get-started/smart-routing","metadata":{"icon":"route","title":null,"description":null,"href":"/v3/get-started/smart-routing","hidden":false}},{"content":"# OpenAI-Compatible Chat Completions\n\nBuild conversational AI applications using Eden AI's OpenAI-compatible chat completions endpoint.\n\n## Overview\n\nEden AI V3 provides full OpenAI API compatibility with multi-provider support. The endpoint follows OpenAI's exact format, making it a drop-in replacement.\n\n**Endpoint:**\n\n```\nPOST /v3/llm/chat/completions\n```\n\n**Note:** Streaming is optional. When enabled, responses are delivered via Server-Sent Events (SSE). See [Streaming Responses](./streaming) for streaming examples.\n\n## Model Format\n\nUse the simplified model string format for LLM:\n\n```\nprovider/model\n```\n\n**Examples:**\n\n* `openai/gpt-4`\n* `anthropic/claude-sonnet-4-5`\n* `google/gemini-pro`\n* `cohere/command-r-plus`\n\n## Basic Chat Completion\n\n<CodeGroup>\n  ```python Python theme={null}\n  import requests\n\n  url = \"https://api.edenai.run/v3/llm/chat/completions\"\n  headers = {\n      \"Authorization\": \"Bearer YOUR_API_KEY\",\n      \"Content-Type\": \"application/json\"\n  }\n\n  payload = {\n      \"model\": \"openai/gpt-4\",\n      \"messages\": [\n          {\"role\": \"user\", \"content\": \"Hello! How are you?\"}\n      ]\n  }\n\n  response = requests.post(url, headers=headers, json=payload)\n  result = response.json()\n  print(result[\"choices\"][0][\"message\"][\"content\"])\n  ```\n\n  ```javascript JavaScript theme={null}\n  const url = 'https://api.edenai.run/v3/llm/chat/completions';\n  const headers = {\n    'Authorization': 'Bearer YOUR_API_KEY',\n    'Content-Type': 'application/json'\n  };\n\n  const payload = {\n    model: 'openai/gpt-4',\n    messages: [{role: 'user', content: 'Hello!'}]\n  };\n\n  const response = await fetch(url, {\n    method: 'POST',\n    headers: headers,\n    body: JSON.stringify(payload)\n  });\n\n  const result = await response.json();\n  console.log(result.choices[0].message.content);\n  ```\n\n  ```bash cURL theme={null}\n  curl -X POST https://api.edenai.run/v3/llm/chat/completions \\\n    -H \"Authorization: Bearer YOUR_API_KEY\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n      \"model\": \"openai/gpt-4\",\n      \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n    }'\n  ```\n</CodeGroup>\n\n## Multi-Turn Conversations\n\nBuild conversations with message history:\n\n<CodeGroup>\n  ```python Python theme={null}\n  import requests\n\n  url = \"https://api.edenai.run/v3/llm/chat/completions\"\n  headers = {\n      \"Authorization\": \"Bearer YOUR_API_KEY\",\n      \"Content-Type\": \"application/json\"\n  }\n\n  payload = {\n      \"model\": \"anthropic/claude-sonnet-4-5\",\n      \"messages\": [\n          {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n          {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n          {\"role\": \"user\", \"content\": \"What's the population?\"}\n      ]\n  }\n\n  response = requests.post(url, headers=headers, json=payload)\n  result = response.json()\n  print(result[\"choices\"][0][\"message\"][\"content\"])\n  ```\n</CodeGroup>\n\n## System Messages\n\nGuide the model's behavior with system messages:\n\n<CodeGroup>\n  ```python Python theme={null}\n  import requests\n\n  url = \"https://api.edenai.run/v3/llm/chat/completions\"\n  headers = {\n      \"Authorization\": \"Bearer YOUR_API_KEY\",\n      \"Content-Type\": \"application/json\"\n  }\n\n  payload = {\n      \"model\": \"openai/gpt-4\",\n      \"messages\": [\n          {\n              \"role\": \"system\",\n              \"content\": \"You are a helpful assistant that speaks like a pirate.\"\n          },\n          {\n              \"role\": \"user\",\n              \"content\": \"Tell me about artificial intelligence.\"\n          }\n      ]\n  }\n\n  response = requests.post(url, headers=headers, json=payload)\n  result = response.json()\n  print(result[\"choices\"][0][\"message\"][\"content\"])\n  ```\n</CodeGroup>\n\n## Temperature and Parameters\n\nControl response creativity and behavior:\n\n<CodeGroup>\n  ```python Python theme={null}\n  import requests\n\n  url = \"https://api.edenai.run/v3/llm/chat/completions\"\n  headers = {\n      \"Authorization\": \"Bearer YOUR_API_KEY\",\n      \"Content-Type\": \"application/json\"\n  }\n\n  payload = {\n      \"model\": \"openai/gpt-4\",\n      \"messages\": [\n          {\"role\": \"user\", \"content\": \"Write a creative story about a robot.\"}\n      ],\n      \"temperature\": 0.9,  # Higher = more creative (0-2)\n      \"max_tokens\": 500    # Limit response length\n  }\n\n  response = requests.post(url, headers=headers, json=payload)\n  result = response.json()\n  print(result[\"choices\"][0][\"message\"][\"content\"])\n  ```\n</CodeGroup>\n\n## Available Parameters\n\n| Parameter           | Type    | Default  | Description                           |\n| ------------------- | ------- | -------- | ------------------------------------- |\n| `model`             | string  | Required | Model string (e.g., `openai/gpt-4`)   |\n| `messages`          | array   | Required | Conversation messages                 |\n| `stream`            | boolean | false    | Enable streaming (uses SSE when true) |\n| `temperature`       | float   | 1.0      | Randomness (0-2)                      |\n| `max_tokens`        | integer | -        | Maximum response tokens               |\n| `top_p`             | float   | 1.0      | Nucleus sampling threshold            |\n| `frequency_penalty` | float   | 0.0      | Penalize repeated tokens (-2 to 2)    |\n| `presence_penalty`  | float   | 0.0      | Penalize topic repetition (-2 to 2)   |\n\n## Response Format\n\nStandard JSON response:\n\n```json  theme={null}\n{\n  \"id\": \"chatcmpl-123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1677652288,\n  \"model\": \"gpt-4\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Hello! I'm doing well, thank you for asking.\"\n      },\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 12,\n    \"completion_tokens\": 15,\n    \"total_tokens\": 27\n  }\n}\n```\n\n## Available Models\n\n### OpenAI\n\n* `openai/gpt-4`\n* `openai/gpt-4-turbo`\n* `openai/gpt-3.5-turbo`\n\n### Anthropic\n\n* `anthropic/claude-sonnet-4-5`\n* `anthropic/claude-opus-4-5`\n* `anthropic/claude-sonnet-4-5`\n\n### Google\n\n* `google/gemini-pro`\n* `google/gemini-1.5-pro`\n\n### Cohere\n\n* `cohere/command-r-plus`\n* `cohere/command-r`\n\n### Meta\n\n* `meta/llama-3-70b`\n* `meta/llama-3-8b`\n\n## OpenAI Python SDK Integration\n\nUse Eden AI with the OpenAI SDK:\n\n<CodeGroup>\n  ```python Python theme={null}\n  from openai import OpenAI\n\n  # Point to Eden AI endpoint\n  client = OpenAI(\n      api_key=\"YOUR_EDEN_AI_API_KEY\",\n      base_url=\"https://api.edenai.run/v3/llm\"\n  )\n\n  # Use any provider through OpenAI SDK\n  response = client.chat.completions.create(\n      model=\"anthropic/claude-sonnet-4-5\",\n      messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n  )\n\n  print(response.choices[0].message.content)\n  ```\n</CodeGroup>\n\n## Next Steps\n\n* [Streaming Responses](./streaming) - Handle Server-Sent Events for real-time output\n* [File Attachments](./file-attachments) - Send images and documents\n* [Working with Media Files](./working-with-media) - Send images and files to LLMs\n","path":"v3/how-to/llm/chat-completions","metadata":{"title":null,"description":null,"href":"/v3/how-to/llm/chat-completions","hidden":false}},{"content":"# Streaming Responses with Server-Sent Events\n\nLearn how to handle streaming responses from the V3 LLM endpoint using Server-Sent Events (SSE).\n\n## Overview\n\nWhen streaming is enabled in V3, LLM responses are delivered via Server-Sent Events (SSE), providing real-time token-by-token output. Streaming is optional - you can also use V3 with non-streaming requests.\n\n**Benefits:**\n\n* Immediate response feedback\n* Better user experience with progressive display\n* Lower perceived latency\n\n## Server-Sent Events Format\n\nSSE responses follow this pattern:\n\n```\ndata: {JSON_CHUNK}\n\ndata: {JSON_CHUNK}\n\ndata: [DONE]\n```\n\nEach line starts with `data: ` followed by JSON or the `[DONE]` marker.\n\n## Parsing Streaming Responses\n\n### Python with requests\n\n<CodeGroup>\n  ```python Python theme={null}\n  import requests\n  import json\n\n  url = \"https://api.edenai.run/v3/llm/chat/completions\"\n  headers = {\n      \"Authorization\": \"Bearer YOUR_API_KEY\",\n      \"Content-Type\": \"application/json\"\n  }\n\n  payload = {\n      \"model\": \"openai/gpt-4\",\n      \"messages\": [{\"role\": \"user\", \"content\": \"Tell me a short story\"}],\n      \"stream\": True\n  }\n\n  response = requests.post(url, headers=headers, json=payload, stream=True)\n\n  full_content = \"\"\n\n  for line in response.iter_lines():\n      if line:\n          line_str = line.decode('utf-8')\n              \n          # Skip empty lines and non-data lines\n          if not line_str.startswith('data: '):\n              continue\n              \n          # Extract data after 'data: ' prefix\n          data = line_str[6:]\n              \n          # Check for end of stream\n          if data == '[DONE]':\n              break\n              \n          # Parse JSON chunk\n          try:\n              chunk = json.loads(data)\n              delta = chunk['choices'][0]['delta']\n                  \n              if 'content' in delta:\n                  content = delta['content']\n                  full_content += content\n                  print(content, end='', flush=True)\n                      \n          except json.JSONDecodeError:\n              continue\n\n  print(f\"\\n\\nFull response: {full_content}\")\n  ```\n</CodeGroup>\n\n### JavaScript with Fetch API\n\n<CodeGroup>\n  ```javascript JavaScript theme={null}\n  const url = 'https://api.edenai.run/v3/llm/chat/completions';\n  const headers = {\n    'Authorization': 'Bearer YOUR_API_KEY',\n    'Content-Type': 'application/json'\n  };\n\n  const payload = {\n    model: 'openai/gpt-4',\n    messages: [{role: 'user', content: 'Tell me a short story'}],\n    stream: true\n  };\n\n  const response = await fetch(url, {\n    method: 'POST',\n    headers: headers,\n    body: JSON.stringify(payload)\n  });\n\n  const reader = response.body.getReader();\n  const decoder = new TextDecoder();\n  let buffer = '';\n\n  while (true) {\n    const {done, value} = await reader.read();\n    if (done) break;\n\n    buffer += decoder.decode(value, {stream: true});\n    const lines = buffer.split('\\n');\n    buffer = lines.pop(); // Keep incomplete line in buffer\n\n    for (const line of lines) {\n      if (line.startsWith('data: ')) {\n        const data = line.slice(6);\n            \n        if (data === '[DONE]') {\n          console.log('\\nStream finished');\n          break;\n        }\n\n        try {\n          const chunk = JSON.parse(data);\n          const content = chunk.choices[0]?.delta?.content;\n          if (content) {\n            process.stdout.write(content);\n          }\n        } catch (e) {\n          // Ignore parse errors\n        }\n      }\n    }\n  }\n  ```\n</CodeGroup>\n\n### Python with OpenAI SDK\n\n<CodeGroup>\n  ```python Python theme={null}\n  from openai import OpenAI\n\n  client = OpenAI(\n      api_key=\"YOUR_EDEN_AI_API_KEY\",\n      base_url=\"https://api.edenai.run/v3/llm\"\n  )\n\n  stream = client.chat.completions.create(\n      model=\"anthropic/claude-sonnet-4-5\",\n      messages=[{\"role\": \"user\", \"content\": \"Tell me a short story\"}],\n      stream=True\n  )\n\n  full_content = \"\"\n\n  for chunk in stream:\n      if chunk.choices[0].delta.content:\n          content = chunk.choices[0].delta.content\n          full_content += content\n          print(content, end='', flush=True)\n\n  print(f\"\\n\\nComplete response: {full_content}\")\n  ```\n</CodeGroup>\n\n## Chunk Structure\n\nEach JSON chunk follows OpenAI's format:\n\n```json  theme={null}\n{\n  \"id\": \"chatcmpl-abc123\",\n  \"object\": \"chat.completion.chunk\",\n  \"created\": 1677652288,\n  \"model\": \"gpt-4\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"delta\": {\n        \"content\": \"Hello\"\n      },\n      \"finish_reason\": null\n    }\n  ]\n}\n```\n\n### Key Fields\n\n| Field                     | Description                                         |\n| ------------------------- | --------------------------------------------------- |\n| `id`                      | Unique completion ID                                |\n| `created`                 | Unix timestamp                                      |\n| `model`                   | Model used                                          |\n| `choices[].delta.role`    | Role (only in first chunk)                          |\n| `choices[].delta.content` | Token content                                       |\n| `choices[].finish_reason` | Stop reason in final chunk (`stop`, `length`, etc.) |\n\n## Handling Different Finish Reasons\n\n<CodeGroup>\n  ```python Python theme={null}\n  import json\n  import requests\n  for line in response.iter_lines():\n      if line:\n          line_str = line.decode('utf-8')\n          if line_str.startswith('data: '):\n              data = line_str[6:]\n                  \n              if data == '[DONE]':\n                  break\n                  \n              chunk = json.loads(data)\n              choice = chunk['choices'][0]\n                  \n              # Check for content\n              if 'content' in choice['delta']:\n                  print(choice['delta']['content'], end='', flush=True)\n                  \n              # Check finish reason\n              finish_reason = choice.get('finish_reason')\n              if finish_reason:\n                  if finish_reason == 'stop':\n                      print(\"\\n[Completed normally]\")\n                  elif finish_reason == 'length':\n                      print(\"\\n[Stopped: max tokens reached]\")\n                  elif finish_reason == 'content_filter':\n                      print(\"\\n[Stopped: content filter]\")\n  ```\n</CodeGroup>\n\n## Error Handling\n\nHandle connection errors and timeouts:\n\n<CodeGroup>\n  ```python Python theme={null}\n  import requests\n  from requests.exceptions import Timeout, ConnectionError\n\n  try:\n      response = requests.post(\n          url,\n          headers=headers,\n          json=payload,\n          stream=True,\n          timeout=60  # 60 second timeout\n      )\n          \n      response.raise_for_status()\n          \n      for line in response.iter_lines():\n          if line:\n              line_str = line.decode('utf-8')\n              # Process line...\n                  \n  except Timeout:\n      print(\"Request timed out\")\n  except ConnectionError:\n      print(\"Connection error\")\n  except requests.exceptions.HTTPError as e:\n      print(f\"HTTP error: {e}\")\n  ```\n</CodeGroup>\n\n## Buffering for UI Display\n\nBuffer tokens for smoother UI updates:\n\n<CodeGroup>\n  ```python Python theme={null}\n  import json\n  import time\n  import requests\n\n  buffer = \"\"\n  last_update = time.time()\n  update_interval = 0.05  # Update UI every 50ms\n\n  for line in response.iter_lines():\n      if line:\n          line_str = line.decode('utf-8')\n          if line_str.startswith('data: '):\n              data = line_str[6:]\n                  \n              if data == '[DONE]':\n                  # Flush remaining buffer\n                  if buffer:\n                      update_ui(buffer)\n                  break\n                  \n              chunk = json.loads(data)\n              if 'content' in chunk['choices'][0]['delta']:\n                  content = chunk['choices'][0]['delta']['content']\n                  buffer += content\n                      \n                  # Update UI periodically\n                  now = time.time()\n                  if now - last_update >= update_interval:\n                      update_ui(buffer)\n                      buffer = \"\"\n                      last_update = now\n\n  def update_ui(text):\n      \"\"\"Update your UI with the buffered text\"\"\"\n      print(text, end='', flush=True)\n  ```\n</CodeGroup>\n\n## React/Frontend Integration\n\nExample React hook for streaming:\n\n<CodeGroup>\n  ```javascript JavaScript theme={null}\n  import { useState, useCallback } from 'react';\n\n  function useStreamingChat() {\n    const [content, setContent] = useState('');\n    const [isStreaming, setIsStreaming] = useState(false);\n\n    const sendMessage = useCallback(async (message) => {\n      setIsStreaming(true);\n      setContent('');\n\n      const response = await fetch('https://api.edenai.run/v3/llm/chat/completions', {\n        method: 'POST',\n        headers: {\n          'Authorization': 'Bearer YOUR_API_KEY',\n          'Content-Type': 'application/json'\n        },\n        body: JSON.stringify({\n          model: 'openai/gpt-4',\n          messages: [{role: 'user', content: message}],\n          stream: true\n        })\n      });\n\n      const reader = response.body.getReader();\n      const decoder = new TextDecoder();\n      let buffer = '';\n\n      while (true) {\n        const {done, value} = await reader.read();\n        if (done) break;\n\n        buffer += decoder.decode(value, {stream: true});\n        const lines = buffer.split('\\n');\n        buffer = lines.pop();\n\n        for (const line of lines) {\n          if (line.startsWith('data: ')) {\n            const data = line.slice(6);\n            if (data === '[DONE]') break;\n\n            try {\n              const chunk = JSON.parse(data);\n              const newContent = chunk.choices[0]?.delta?.content;\n              if (newContent) {\n                setContent(prev => prev + newContent);\n              }\n            } catch (e) {}\n          }\n        }\n      }\n\n      setIsStreaming(false);\n    }, []);\n\n    return { content, isStreaming, sendMessage };\n  }\n  ```\n</CodeGroup>\n\n## Next Steps\n\n* [Chat Completions](./chat-completions) - Basic chat setup\n* [File Attachments](./file-attachments) - Send images to LLMs\n* [Getting Started](../../get-started/introduction) - V3 basics\n","path":"v3/how-to/llm/streaming","metadata":{"title":null,"description":null,"href":"/v3/how-to/llm/streaming","hidden":false}}]}}

data: {"type":"finish-step"}

data: {"type":"start-step"}

data: {"type":"text-start","id":"gen-1772182350-1JESDTwKP6guG7ux09o2"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"To"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" make"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" a"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" chat"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" completion"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" request"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" with Eden"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" AI,"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" send"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" a `"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"POST"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"` request to `"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"https://api.edenai."}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"run/v3/llm/"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"chat/completions`"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" with your"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" API key and a"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" model"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" string"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" in"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" `"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"provider"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"/"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"model` format."}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"\n\n**"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"Example ("}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"Python):**\n```python\nimport"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" requests"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"\n\nresponse"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" = requests.post(\n    \""}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"https://api.edenai."}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"run/v3/llm/"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"chat/completions\",\n    headers"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"={\n        \"Authorization\": \"Bearer"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" YOUR_API"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"_KEY\",\n        \"Content-"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"Type\": \"application/json\""}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"\n    },\n    json={\n        "}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"\"model\": \"openai/g"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"pt-4\",\n        \"messages"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"\": [{\"role\": \"user\","}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" \"content\": \"Hello!\"}]"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"\n    }\n)\nprint"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"(response.json()"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"[\"choices\"][0][\"message\"][\"content"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"\"])\n```\n\nYou"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" can"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" also use the **"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"OpenAI Python"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" SDK** by"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" pointing"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" `"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"base"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"_url` to `https://api"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":".edenai.run/v"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"3/llm`."}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" Supported"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" providers"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" include Open"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"AI, Anthropic, Google"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":","}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" Cohere, and more"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"."}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" Optional"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" parameters"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" include `temperature"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"`, `max_tokens`, `"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"stream"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"`, and others"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":".\n\nFor"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" more"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" details"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":", check"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" these"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" pages"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":":"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"\n\n```"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"suggestions"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"\n("}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"Chat"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" Completions Guide"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":")[/v3/how-to"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"/llm/chat-completions"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"]\n("}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"Streaming Responses)[/v3/"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"how-to/llm/streaming"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"]\n(Getting"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" Started with"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":" V3)[/v3/get"}

data: {"type":"text-delta","id":"gen-1772182350-1JESDTwKP6guG7ux09o2","delta":"-started/introduction]\n```"}

data: {"type":"text-end","id":"gen-1772182350-1JESDTwKP6guG7ux09o2"}

data: {"type":"finish-step"}

data: {"type":"finish","finishReason":"stop"}

data: [DONE]

