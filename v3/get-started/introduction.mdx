---
sidebarTitle: "Introduction"
icon: "book-open"
---
# Getting Started with Eden AI V3 API

Welcome to the Eden AI V3 API! This guide will help you understand V3's unified architecture and make your first API calls.

## Overview

Eden AI V3 introduces a revolutionary approach to AI API integration with:

- **Universal AI Endpoint** - Single endpoint for all non-LLM features
- **OpenAI-Compatible Format** - Drop-in replacement for OpenAI's API
- **Persistent File Storage** - Upload once, use in multiple requests
- **Built-in API Discovery** - Explore features and schemas programmatically
- **Mandatory Streaming** - All LLM responses use Server-Sent Events (SSE)

<Note>If you were an user before 2026/01/05, you still have access to the previous version: [https://old-app.edenai.run/](https://old-app.edenai.run/). We'll continue supporting the old version until the end of 2026. It your're looking for the documentation, you can find it [here](https://old-docs.edenai.co)</Note>

## V3 Architecture

V3 uses a **model string** format instead of separate provider parameters:

```
feature/subfeature/provider[/model]
```

**Examples:**
- `text/ai_detection/openai/gpt-4`
- `ocr/financial_parser/google`
- `image/generation/openai/dall-e-3`
- `openai/gpt-4` (for LLM endpoints)

This unified format allows a single endpoint to handle all features intelligently.

## Prerequisites

Before you start, you'll need:

1. **API Token** - Get your token from the [Eden AI dashboard](https://app.edenai.run/)
2. **HTTP Client** - Use cURL, Python requests, or any HTTP client
3. **Credits** - Ensure your account has sufficient credits

## Base URL

All V3 API endpoints are available at:

```
https://api.edenai.run/v3
```

## Authentication

All requests must include your API token in the Authorization header:

<CodeGroup>
```bash cURL
curl -X POST https://api.edenai.run/v3/universal-ai \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json"
    ```

```python Python
import requests

headers = {
    "Authorization": "Bearer YOUR_API_KEY",
    "Content-Type": "application/json"
}

response = requests.post(
    "https://api.edenai.run/v3/universal-ai",
    headers=headers,
    json={"model": "text/moderation/openai", "input": {"text": "Sample"}}
)
    ```

```javascript JavaScript
const headers = {
  'Authorization': 'Bearer YOUR_API_KEY',
  'Content-Type': 'application/json'
};

const response = await fetch('https://api.edenai.run/v3/universal-ai', {
  method: 'POST',
  headers: headers,
  body: JSON.stringify({
    model: 'text/moderation/openai',
    input: { text: 'Sample' }
  })
});
    ```
</CodeGroup>

## Your First API Call: Universal AI

The Universal AI endpoint handles all non-LLM features through a single endpoint. Let's analyze some text:

<CodeGroup>
```bash cURL
curl -X POST https://api.edenai.run/v3/universal-ai \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "text/ai_detection/openai/gpt-4",
    "input": {
      "text": "This is a sample text to check if it was AI-generated."
    }
  }'
    ```

```python Python
import requests

url = "https://api.edenai.run/v3/universal-ai"
headers = {
    "Authorization": "Bearer YOUR_API_KEY",
    "Content-Type": "application/json"
}
payload = {
    "model": "text/ai_detection/openai/gpt-4",
    "input": {
        "text": "This is a sample text to check if it was AI-generated."
    }
}

response = requests.post(url, headers=headers, json=payload)
result = response.json()
print(result)
    ```

```javascript JavaScript
const url = 'https://api.edenai.run/v3/universal-ai';
const headers = {
  'Authorization': 'Bearer YOUR_API_KEY',
  'Content-Type': 'application/json'
};
const payload = {
  model: 'text/ai_detection/openai/gpt-4',
  input: {
    text: 'This is a sample text to check if it was AI-generated.'
  }
};

const response = await fetch(url, {
  method: 'POST',
  headers: headers,
  body: JSON.stringify(payload)
});

const result = await response.json();
console.log(result);
    ```
</CodeGroup>

### Response Format

All V3 responses follow a consistent structure:

```json
{
  "status": "success",
  "cost": 0.0001,
  "provider": "openai",
  "feature": "text",
  "subfeature": "ai_detection",
  "output": {
    "ai_score": 0.85,
    "is_ai_generated": true
  }
}
```

## Your First LLM Call: OpenAI-Compatible

The LLM endpoint provides OpenAI-compatible chat completions with **mandatory streaming**:

<CodeGroup>
```python Python
import requests

url = "https://api.edenai.run/v3/llm/chat/completions"
headers = {
    "Authorization": "Bearer YOUR_API_KEY",
    "Content-Type": "application/json"
}
payload = {
    "model": "openai/gpt-4",
    "messages": [
        {"role": "user", "content": "Hello! How are you?"}
    ],
    "stream": True
}

# Stream response with SSE
response = requests.post(url, headers=headers, json=payload, stream=True)

for line in response.iter_lines():
    if line:
        print(line.decode('utf-8'))
    ```

```bash cURL
curl -X POST https://api.edenai.run/v3/llm/chat/completions \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-4",
    "messages": [{"role": "user", "content": "Hello!"}],
    "stream": true
  }'
    ```

```javascript JavaScript
const url = 'https://api.edenai.run/v3/llm/chat/completions';
const headers = {
  'Authorization': 'Bearer YOUR_API_KEY',
  'Content-Type': 'application/json'
};
const payload = {
  model: 'openai/gpt-4',
  messages: [{role: 'user', content: 'Hello!'}],
  stream: true
};

const response = await fetch(url, {
  method: 'POST',
  headers: headers,
  body: JSON.stringify(payload)
});

const reader = response.body.getReader();
const decoder = new TextDecoder();

while (true) {
  const {done, value} = await reader.read();
  if (done) break;
  console.log(decoder.decode(value));
}
    ```
</CodeGroup>

### Streaming Response Format

LLM responses use Server-Sent Events (SSE):

```
data: {"id":"chatcmpl-123","object":"chat.completion.chunk","choices":[{"delta":{"content":"Hello"},"index":0}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","choices":[{"delta":{"content":"!"},"index":0}]}

data: [DONE]
```

## Working with Files

V3 introduces **persistent file storage**. Upload files once, then reference them by ID:

### Step 1: Upload a File

<CodeGroup>
```python Python
import requests

url = "https://api.edenai.run/v3/upload"
headers = {"Authorization": "Bearer YOUR_API_KEY"}

files = {"file": open("document.pdf", "rb")}
data = {"purpose": "ocr-processing"}

response = requests.post(url, headers=headers, files=files, data=data)
file_info = response.json()
file_id = file_info["file_id"]

print(f"Uploaded file ID: {file_id}")
# Output: Uploaded file ID: 550e8400-e29b-41d4-a716-446655440000
    ```

```bash cURL
curl -X POST https://api.edenai.run/v3/upload \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -F "file=@document.pdf" \
  -F "purpose=ocr-processing"
    ```
</CodeGroup>

### Step 2: Use the File in Requests

<CodeGroup>
```python Python
import requests
# Use the file_id in a Universal AI request
url = "https://api.edenai.run/v3/universal-ai"
headers = {
    "Authorization": "Bearer YOUR_API_KEY",
    "Content-Type": "application/json"
}
payload = {
    "model": "ocr/financial_parser/google",
    "input": {
        "file": "550e8400-e29b-41d4-a716-446655440000"
    }
}

response = requests.post(url, headers=headers, json=payload)
result = response.json()
print(result["output"]["text"])
    ```
</CodeGroup>

## Understanding Model Strings

The model string is the key to V3's unified architecture. Here's the format:

```
feature/subfeature/provider[/model]
```

### Breaking It Down

| Component | Description | Example |
|-----------|-------------|---------|
| `feature` | Category of AI capability | `text`, `ocr`, `image`, `translation` |
| `subfeature` | Specific functionality | `ai_detection`, `moderation`, `ocr`, `generation` |
| `provider` | AI provider | `openai`, `google`, `amazon`, `anthropic` |
| `model` | Specific model (optional) | `gpt-4`, `claude-3-5-sonnet`, `gemini-pro` |

### Examples

**Text Analysis:**
- `text/ai_detection/openai/gpt-4` - Detect AI-generated text with GPT-4
- `text/moderation/openai` - Content moderation with OpenAI's default model
- `text/embeddings/cohere/embed-english-v3.0` - Generate embeddings with Cohere

**OCR:**
- `ocr/financial_parser/google` - Extract financial information with Google DocumentAI
- `ocr/identity_parser/amazon` - Parse ID documents with Amazon Textract
- `ocr/invoice_parser/microsoft` - Extract invoice data with Azure

**Image:**
- `image/generation/openai/dall-e-3` - Generate images with DALL-E 3
- `image/object_detection/google` - Detect objects with Google Vision
- `image/face_detection/amazon` - Detect faces with AWS Rekognition

**LLM (simplified format):**
- `openai/gpt-4` - Chat with GPT-4
- `anthropic/claude-3-5-sonnet-20241022` - Chat with Claude
- `google/gemini-pro` - Chat with Gemini

## API Discovery

V3 includes built-in endpoints to explore available features programmatically:

<CodeGroup>
```python Python
import requests

# List all features
response = requests.get(
    "https://api.edenai.run/v3/info"
)
features = response.json()
print(features)
# Output: {"text": ["ai_detection", "moderation", ...], "ocr": [...], ...}

# Get details about a specific feature
response = requests.get(
    "https://api.edenai.run/v3/info/text/ai_detection"
)
feature_info = response.json()
print(feature_info["providers"])  # List of available providers
print(feature_info["input_schema"])  # Expected input format
print(feature_info["output_schema"])  # Response format
    ```

```bash cURL
# List all features
curl https://api.edenai.run/v3/info 
# Get feature details
curl https://api.edenai.run/v3/info/text/ai_detection 
    ```
</CodeGroup>

## Error Handling

V3 uses standard HTTP status codes with detailed error messages:

```json
{
  "status": "error",
  "error": {
    "code": "invalid_model_string",
    "message": "Model string format must be feature/subfeature/provider[/model]",
    "details": {
      "provided": "invalid/model",
      "expected": "feature/subfeature/provider[/model]"
    }
  }
}
```

### Common Status Codes

- `200` - Success
- `400` - Bad Request (invalid model string or input)
- `401` - Unauthorized (invalid API token)
- `402` - Payment Required (insufficient credits)
- `404` - Not Found (feature/provider not available)
- `422` - Validation Error (invalid request body)
- `429` - Rate Limit Exceeded
- `500` - Internal Server Error

## V3 vs V2: Key Differences

| Aspect | V2 | V3 |
|--------|----|----|
| **Endpoints** | Feature-specific (`/v2/text/moderation`) | Universal (`/v3/universal-ai`) + LLM |
| **Provider Format** | `providers` parameter | Model string (`text/moderation/openai`) |
| **File Handling** | Per-request uploads | Persistent storage with file IDs |
| **LLM Streaming** | Optional | Mandatory (always SSE) |
| **API Discovery** | Documentation only | Built-in `/v3/info` endpoints |
| **OpenAI Compatibility** | Custom format | Native OpenAI format |

## Next Steps

Now that you understand V3 basics, explore specific features:

### Universal AI
- [Getting Started with Universal AI](../how-to/universal-ai/getting-started) - Learn the universal endpoint
- [Text Features](../how-to/universal-ai/text-features) - AI detection, moderation, embeddings
- [OCR Features](../how-to/universal-ai/ocr-features) - Text extraction, document parsing
- [Image Features](../how-to/universal-ai/image-features) - Generation, detection, analysis

### OpenAI-Compatible LLM
- [Chat Completions](../how-to/llm/chat-completions) - Build conversational AI with streaming
- [Streaming Responses](../how-to/llm/streaming) - Handle Server-Sent Events
- [File Attachments](../how-to/llm/file-attachments) - Send images and documents to LLMs

### File Management
- [Upload Files](../how-to/upload/upload-files) - Persistent file storage

### API Discovery
- [Explore the API](../how-to/discovery/explore-api) - Programmatic feature discovery

## Need Help?

- Visit [Eden AI Support](https://edenai.co/) for additional assistance
