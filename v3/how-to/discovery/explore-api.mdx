# Explore the API

Use V3's built-in API discovery endpoints to programmatically explore available features, providers, and schemas.

## Overview

V3 provides `/v3/info` endpoints that let you discover:
- Available features and subfeatures
- Available models and pricing for each feature
- Input/output schemas

**Base Endpoint:**
```
GET /v3/info
```

## List All Features

Get a complete list of available features:

<CodeGroup>
```python Python
import requests

url = "https://api.edenai.run/v3/info"

response = requests.get(url)
data = response.json()

for feature in data["features"]:
    subfeature_names = [sf["name"] for sf in feature["subfeatures"]]
    print(f"{feature['name']}: {subfeature_names}")
    ```

```bash cURL
curl https://api.edenai.run/v3/info \
    ```
</CodeGroup>

## Explore a Specific Feature

Get details about a feature category:

<CodeGroup>
```python Python
import requests
# Get all text subfeatures
url = "https://api.edenai.run/v3/info/text"
response = requests.get(url)
text_info = response.json()

print("Available text subfeatures:")
for subfeature in text_info["subfeatures"]:
    print(f"  - {subfeature['name']}: {subfeature['fullname']}")
    ```
</CodeGroup>

## Get Feature Details

Retrieve complete information about a specific feature:

<CodeGroup>
```python Python
import requests
# Get details about content moderation
url = "https://api.edenai.run/v3/info/text/moderation"
headers = {"Authorization": "Bearer YOUR_API_KEY"}
response = requests.get(url, headers=headers)
feature_info = response.json()

print("Feature: text/moderation")
print(f"\nAvailable models: {feature_info['models']}")
print(f"\nInput schema:")
print(feature_info['input_schema'])
print(f"\nOutput schema:")
print(feature_info['output_schema'])
    ```
</CodeGroup>

### Response Example

```json
{
  "feature": "text",
  "subfeature": "moderation",
  "description": "...",
  "mode": "sync",
  "endpoints": {"create": "POST /v3/universal-ai"},
  "input_schema": {
    "fields": [
      {
        "name": "text",
        "required": true,
        "description": "Text to moderate for harmful content",
        "type": "string"
      }
    ]
  },
  "output_schema": {
    "fields": [
      {"name": "nsfw_likelihood", "required": true, "type": "integer"},
      {"name": "items", "required": false, "type": "array"},
      {"name": "nsfw_likelihood_score", "required": true, "type": "float"}
    ]
  },
  "models": [
    {
      "model": "text/moderation/google",
      "pricing": {"price": 1.0, "price_unit_quantity": 1000, "price_unit_type": "request"}
    },
    {
      "model": "text/moderation/openai",
      "pricing": {"price": 0.5, "price_unit_quantity": 1000, "price_unit_type": "request"}
    }
  ]
}
```

## Check Available Models

List all available models for a specific feature, along with their pricing:

<CodeGroup>
```python Python
import requests
def get_models_for_feature(feature, subfeature):
    """Get available models and pricing for a feature"""

    url = f"https://api.edenai.run/v3/info/{feature}/{subfeature}"

    response = requests.get(url)
    info = response.json()

    for entry in info["models"]:
        model = entry["model"]
        pricing = entry["pricing"]
        price = pricing["price"]
        unit_qty = pricing["price_unit_quantity"]
        unit_type = pricing["price_unit_type"]
        print(f"  {model}: ${price}/{unit_qty} {unit_type}s")

# Usage
print("OCR models:")
get_models_for_feature("ocr", "ocr")
    ```
</CodeGroup>

## Validate Model Strings

Check if a model string is valid before making a request:

<CodeGroup>
```python Python
import requests
def validate_model_string(model_string):
    """Validate model string format and availability"""

    # Parse model string
    parts = model_string.split('/')

    if len(parts) < 3:
        return {"valid": False, "error": "Invalid format"}

    feature = parts[0]
    subfeature = parts[1]

    # Check feature availability
    url = f"https://api.edenai.run/v3/info/{feature}/{subfeature}"

    try:
        response = requests.get(url)
        response.raise_for_status()
        info = response.json()

        # Check if the model string exists in available models
        available_models = [m["model"] for m in info["models"]]

        if model_string not in available_models:
            return {
                "valid": False,
                "error": f"Model '{model_string}' not available",
                "available": available_models
            }

        return {"valid": True, "info": info}

    except Exception as e:
        return {"valid": False, "error": str(e)}

# Usage
result = validate_model_string("text/moderation/google")
if result["valid"]:
    print("Model string is valid!")
else:
    print(f"Invalid: {result['error']}")
    ```
</CodeGroup>

## Build Dynamic UIs

Use discovery to build dynamic feature selection:

<CodeGroup>
```python Python
import requests
def build_feature_menu():
    """Build interactive feature selection menu"""

    # Get all features (includes models inline)
    url = "https://api.edenai.run/v3/info"
    response = requests.get(url)
    data = response.json()

    menu = {}

    for feature in data["features"]:
        feature_name = feature["name"]
        menu[feature_name] = {}

        for subfeature in feature["subfeatures"]:
            sf_name = subfeature["name"]
            # Extract unique providers from model strings
            providers = list({
                m["model"].split("/")[2] for m in subfeature["models"]
            })
            menu[feature_name][sf_name] = providers

    return menu

# Build menu
menu = build_feature_menu()

# Display
for feature, subfeatures in menu.items():
    print(f"\n{feature.upper()}:")
    for subfeature, providers in subfeatures.items():
        print(f"  {subfeature}: {', '.join(providers)}")
    ```
</CodeGroup>

## Cache Discovery Results

Cache API info to reduce requests:

<CodeGroup>
```python Python
import json
import requests
from pathlib import Path
from datetime import datetime, timedelta

class APIDiscoveryCache:
    def __init__(self, cache_dir="api_cache", ttl_hours=24):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.ttl = timedelta(hours=ttl_hours)
        self.headers = {"Authorization": "Bearer YOUR_API_KEY"}
        
    def get_feature_info(self, feature, subfeature):
        """Get feature info with caching"""
            
        cache_key = f"{feature}_{subfeature}"
        cache_file = self.cache_dir / f"{cache_key}.json"
            
        # Check cache
        if cache_file.exists():
            data = json.loads(cache_file.read_text())
            cached_at = datetime.fromisoformat(data["cached_at"])
                
            if datetime.now() - cached_at < self.ttl:
                return data["info"]
            
        # Fetch from API
        url = f"https://api.edenai.run/v3/info/{feature}/{subfeature}"
        response = requests.get(url)
        info = response.json()
            
        # Cache result
        cache_data = {
            "info": info,
            "cached_at": datetime.now().isoformat()
        }
        cache_file.write_text(json.dumps(cache_data, indent=2))
            
        return info

# Usage
cache = APIDiscoveryCache()
info = cache.get_feature_info("text", "moderation")
    ```
</CodeGroup>

## Next Steps

- [Universal AI Getting Started](../universal-ai/getting-started) - Use discovered features
- [Chat Completions Guide](../llm/chat-completions) - LLM endpoint
