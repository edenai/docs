# Python (OpenAI SDK)

Use the official OpenAI Python SDK with Eden AI to access 200+ AI models through a familiar interface.

## Overview

The OpenAI Python SDK is fully compatible with Eden AI's V3 API. Simply point the SDK to Eden AI's endpoint and you can access models from OpenAI, Anthropic, Google, Cohere, Meta, and more.

## Installation

Install the OpenAI Python SDK:

<CodeBlocks>
  <CodeBlock title="pip">
    ```bash
    pip install openai
    ```
  </CodeBlock>

  <CodeBlock title="poetry">
    ```bash
    poetry add openai
    ```
  </CodeBlock>
</CodeBlocks>

## Quick Start

Configure the OpenAI client to use Eden AI:

<CodeBlocks>
  <CodeBlock title="Python">
    ```python
    from openai import OpenAI

    # Initialize client with Eden AI endpoint
    client = OpenAI(
        api_key="YOUR_EDEN_AI_API_KEY",  # Get from https://app.edenai.run
        base_url="https://api.edenai.run/v3/llm"
    )

    # Use streaming (mandatory in V3)
    stream = client.chat.completions.create(
        model="openai/gpt-4",
        messages=[
            {"role": "user", "content": "Hello! How are you?"}
        ],
        stream=True
    )

    # Print the response
    for chunk in stream:
        if chunk.choices[0].delta.content:
            print(chunk.choices[0].delta.content, end='')
    ```
  </CodeBlock>
</CodeBlocks>

## Available Models

Access models from multiple providers using the `provider/model` format:

### OpenAI
- `openai/gpt-4`
- `openai/gpt-4-turbo`
- `openai/gpt-4o`
- `openai/gpt-3.5-turbo`

### Anthropic
- `anthropic/claude-3-5-sonnet-20241022`
- `anthropic/claude-3-opus-20240229`
- `anthropic/claude-3-sonnet-20240229`
- `anthropic/claude-3-haiku-20240307`

### Google
- `google/gemini-pro`
- `google/gemini-1.5-pro`
- `google/gemini-1.5-flash`

### Cohere
- `cohere/command-r-plus`
- `cohere/command-r`

### Meta
- `meta/llama-3-70b`
- `meta/llama-3-8b`

## Multi-Turn Conversations

Build conversational applications with message history:

<CodeBlocks>
  <CodeBlock title="Python">
    ```python
    from openai import OpenAI

    client = OpenAI(
        api_key="YOUR_EDEN_AI_API_KEY",
        base_url="https://api.edenai.run/v3/llm"
    )

    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is the capital of France?"}
    ]

    # First interaction
    stream = client.chat.completions.create(
        model="anthropic/claude-3-5-sonnet-20241022",
        messages=messages,
        stream=True
    )

    assistant_response = ""
    for chunk in stream:
        if chunk.choices[0].delta.content:
            content = chunk.choices[0].delta.content
            print(content, end='')
            assistant_response += content

    # Add assistant response to history
    messages.append({"role": "assistant", "content": assistant_response})

    # Continue conversation
    messages.append({"role": "user", "content": "What's the population?"})

    stream = client.chat.completions.create(
        model="anthropic/claude-3-5-sonnet-20241022",
        messages=messages,
        stream=True
    )

    for chunk in stream:
        if chunk.choices[0].delta.content:
            print(chunk.choices[0].delta.content, end='')
    ```
  </CodeBlock>
</CodeBlocks>

## Advanced Parameters

Control model behavior with standard OpenAI parameters:

<CodeBlocks>
  <CodeBlock title="Python">
    ```python
    from openai import OpenAI

    client = OpenAI(
        api_key="YOUR_EDEN_AI_API_KEY",
        base_url="https://api.edenai.run/v3/llm"
    )

    stream = client.chat.completions.create(
        model="openai/gpt-4",
        messages=[
            {"role": "user", "content": "Write a creative story about AI."}
        ],
        temperature=0.9,           # Higher = more creative (0-2)
        max_tokens=500,            # Limit response length
        top_p=1.0,                 # Nucleus sampling
        frequency_penalty=0.0,     # Penalize repetition (-2 to 2)
        presence_penalty=0.0,      # Penalize topic repetition (-2 to 2)
        stream=True
    )

    for chunk in stream:
        if chunk.choices[0].delta.content:
            print(chunk.choices[0].delta.content, end='')
    ```
  </CodeBlock>
</CodeBlocks>

## Vision Capabilities

Send images to vision-capable models:

<CodeBlocks>
  <CodeBlock title="Python">
    ```python
    from openai import OpenAI

    client = OpenAI(
        api_key="YOUR_EDEN_AI_API_KEY",
        base_url="https://api.edenai.run/v3/llm"
    )

    # First, upload the image to get a file_id
    import requests

    upload_response = requests.post(
        "https://api.edenai.run/v3/upload",
        headers={"Authorization": f"Bearer YOUR_EDEN_AI_API_KEY"},
        files={"file": open("image.jpg", "rb")}
    )
    file_id = upload_response.json()["file_id"]

    # Use the file_id in a chat message
    stream = client.chat.completions.create(
        model="openai/gpt-4o",
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "What's in this image?"},
                    {"type": "file", "file": {"file_id": file_id}}
                ]
            }
        ],
        stream=True
    )

    for chunk in stream:
        if chunk.choices[0].delta.content:
            print(chunk.choices[0].delta.content, end='')
    ```
  </CodeBlock>
</CodeBlocks>

## Async Support

Use async/await for concurrent requests:

<CodeBlocks>
  <CodeBlock title="Python">
    ```python
    from openai import AsyncOpenAI
    import asyncio

    async def main():
        client = AsyncOpenAI(
            api_key="YOUR_EDEN_AI_API_KEY",
            base_url="https://api.edenai.run/v3/llm"
        )

        stream = await client.chat.completions.create(
            model="openai/gpt-4",
            messages=[
                {"role": "user", "content": "Hello!"}
            ],
            stream=True
        )

        async for chunk in stream:
            if chunk.choices[0].delta.content:
                print(chunk.choices[0].delta.content, end='')

    asyncio.run(main())
    ```
  </CodeBlock>
</CodeBlocks>

## Error Handling

Handle API errors gracefully:

<CodeBlocks>
  <CodeBlock title="Python">
    ```python
    from openai import OpenAI, OpenAIError
    import openai

    client = OpenAI(
        api_key="YOUR_EDEN_AI_API_KEY",
        base_url="https://api.edenai.run/v3/llm"
    )

    try:
        stream = client.chat.completions.create(
            model="openai/gpt-4",
            messages=[{"role": "user", "content": "Hello!"}],
            stream=True
        )

        for chunk in stream:
            if chunk.choices[0].delta.content:
                print(chunk.choices[0].delta.content, end='')

    except openai.AuthenticationError as e:
        print(f"Authentication failed: {e}")
    except openai.RateLimitError as e:
        print(f"Rate limit exceeded: {e}")
    except openai.APIError as e:
        print(f"API error: {e}")
    except Exception as e:
        print(f"Unexpected error: {e}")
    ```
  </CodeBlock>
</CodeBlocks>

## Complete Example

A full example with conversation management:

<CodeBlocks>
  <CodeBlock title="Python">
    ```python
    from openai import OpenAI
    import sys

    def chat_with_ai():
        client = OpenAI(
            api_key="YOUR_EDEN_AI_API_KEY",
            base_url="https://api.edenai.run/v3/llm"
        )

        messages = [
            {"role": "system", "content": "You are a helpful assistant."}
        ]

        print("Chat with AI (type 'quit' to exit)")
        print("-" * 50)

        while True:
            # Get user input
            user_input = input("\nYou: ").strip()

            if user_input.lower() == 'quit':
                break

            if not user_input:
                continue

            # Add user message
            messages.append({"role": "user", "content": user_input})

            # Get AI response
            print("\nAssistant: ", end='')
            stream = client.chat.completions.create(
                model="anthropic/claude-3-5-sonnet-20241022",
                messages=messages,
                temperature=0.7,
                stream=True
            )

            assistant_response = ""
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    print(content, end='', flush=True)
                    assistant_response += content

            print()  # New line after response

            # Add assistant response to history
            messages.append({"role": "assistant", "content": assistant_response})

    if __name__ == "__main__":
        chat_with_ai()
    ```
  </CodeBlock>
</CodeBlocks>

## List Available Models

Discover available models programmatically:

<CodeBlocks>
  <CodeBlock title="Python">
    ```python
    import requests

    response = requests.get(
        "https://api.edenai.run/v3/llm/models",
        headers={"Authorization": "Bearer YOUR_EDEN_AI_API_KEY"}
    )

    models = response.json()

    # Print models by provider
    providers = {}
    for model in models["data"]:
        provider = model["owned_by"]
        if provider not in providers:
            providers[provider] = []
        providers[provider].append(model["id"])

    for provider, model_list in providers.items():
        print(f"\n{provider}:")
        for model_id in model_list:
            print(f"  - {model_id}")
    ```
  </CodeBlock>
</CodeBlocks>

## Environment Variables

Store your API key securely using environment variables:

<CodeBlocks>
  <CodeBlock title="Python">
    ```python
    import os
    from openai import OpenAI

    client = OpenAI(
        api_key=os.getenv("EDEN_AI_API_KEY"),
        base_url="https://api.edenai.run/v3/llm"
    )
    ```
  </CodeBlock>

  <CodeBlock title=".env">
    ```bash
    EDEN_AI_API_KEY=your_api_key_here
    ```
  </CodeBlock>
</CodeBlocks>

Use with `python-dotenv`:

<CodeBlocks>
  <CodeBlock title="Python">
    ```python
    from dotenv import load_dotenv
    import os
    from openai import OpenAI

    load_dotenv()

    client = OpenAI(
        api_key=os.getenv("EDEN_AI_API_KEY"),
        base_url="https://api.edenai.run/v3/llm"
    )
    ```
  </CodeBlock>
</CodeBlocks>

## Troubleshooting

### Authentication Errors

Ensure your API key is correct and has the `Bearer` prefix when using raw requests:

```python
headers = {"Authorization": f"Bearer {api_key}"}
```

### Rate Limiting

If you hit rate limits, implement exponential backoff:

<CodeBlocks>
  <CodeBlock title="Python">
    ```python
    import time
    from openai import OpenAI, RateLimitError

    client = OpenAI(
        api_key="YOUR_EDEN_AI_API_KEY",
        base_url="https://api.edenai.run/v3/llm"
    )

    def create_completion_with_retry(messages, max_retries=3):
        for attempt in range(max_retries):
            try:
                return client.chat.completions.create(
                    model="openai/gpt-4",
                    messages=messages,
                    stream=True
                )
            except RateLimitError:
                if attempt < max_retries - 1:
                    wait_time = 2 ** attempt  # Exponential backoff
                    print(f"Rate limited. Waiting {wait_time}s...")
                    time.sleep(wait_time)
                else:
                    raise
    ```
  </CodeBlock>
</CodeBlocks>

### Streaming Issues

Make sure `stream=True` is set:

```python
# Correct
stream = client.chat.completions.create(..., stream=True)

# V3 requires streaming
```

## Next Steps

- [Vision Capabilities](../../how-to/llm/vision-capabilities) - Working with images
- [File Attachments](../../how-to/llm/file-attachments) - Uploading files
- [Provider Comparison](../../how-to/llm/provider-comparison) - Choosing the right model
- [API Reference](/v3-api/reference) - Complete API documentation
